{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Hands-On with LangChain (2025 Edition)\n",
    "\n",
    "This notebook demonstrates LangChain's **2025 capabilities** using Azure OpenAI. We'll explore modern LCEL patterns, LangGraph agents (replacing legacy AgentExecutor), Azure Application Insights  integration, and evaluation frameworks.\n",
    "\n",
    "## üÜï What's New in 2025:\n",
    "- **LangGraph** replaces legacy AgentExecutor patterns\n",
    "- **Azure Application Insights** production-ready monitoring and evaluation  \n",
    "- **LangChain Sandbox** for safe code execution\n",
    "- **Modern evaluation frameworks** integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "**What this does:** Securely loads API keys and configuration from a `.env` file and validates all required settings for the 2025 LangChain stack.\n",
    "\n",
    "The code below creates a comprehensive configuration manager that:\n",
    "- **Loads environment variables** using `python-dotenv` (industry standard for secure credential management)\n",
    "- **Validates Azure OpenAI credentials** (API key, endpoint, deployment name)\n",
    "- **Configures Azure Application Insights + OpenTelemetry for monitoring** for production observability\n",
    "- **Sets up AI Search** for web search capabilities\n",
    "- **Enables 2025 evaluation frameworks** like Azure ML and MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ac5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and configuration setup with 2025 LangSmith integration\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "class NotebookConfig:\n",
    "    \"\"\"Azure-centric configuration management for Jupyter notebooks\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        env_path = find_dotenv()\n",
    "        if env_path:\n",
    "            load_dotenv(env_path)\n",
    "            print(f\"‚úÖ Loaded environment from: {env_path}\")\n",
    "        else:\n",
    "            warnings.warn(\"No .env file found. Using system environment variables only.\")\n",
    "        \n",
    "        self._load_azure_config()\n",
    "        self._load_monitoring_config()\n",
    "        self._validate_config()\n",
    "\n",
    "    def _load_azure_config(self):\n",
    "        \"\"\"Load Azure service configurations\"\"\"\n",
    "        self.azure_openai_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "        self.azure_openai_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "        self.azure_openai_deployment = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME', 'gpt-4o-mini')\n",
    "        self.azure_openai_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-05-01-preview')\n",
    "\n",
    "        self.azure_ai_search_key = os.getenv('AZURE_AI_SEARCH_KEY')\n",
    "        self.azure_ai_search_endpoint = os.getenv('AZURE_AI_SEARCH_ENDPOINT')\n",
    "        self.azure_ai_search_index = os.getenv('AZURE_AI_SEARCH_INDEX')\n",
    "\n",
    "        self.azure_ml_workspace = os.getenv('AZURE_ML_WORKSPACE')\n",
    "        self.azure_ml_subscription_id = os.getenv('AZURE_ML_SUBSCRIPTION_ID')\n",
    "        self.azure_ml_resource_group = os.getenv('AZURE_ML_RESOURCE_GROUP')\n",
    "\n",
    "    def _load_monitoring_config(self):\n",
    "        \"\"\"Load Azure Application Insights configuration\"\"\"\n",
    "        self.app_insights_connection_string = os.getenv('APPINSIGHTS_CONNECTION_STRING')\n",
    "        self.environment = os.getenv('ENVIRONMENT', 'development')\n",
    "        self.debug = os.getenv('DEBUG', 'false').lower() == 'true'\n",
    "\n",
    "    def _validate_config(self):\n",
    "        \"\"\"Validate required Azure configurations\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        if not self.azure_openai_key:\n",
    "            errors.append(\"AZURE_OPENAI_API_KEY is required\")\n",
    "        if not self.azure_openai_endpoint:\n",
    "            errors.append(\"AZURE_OPENAI_ENDPOINT is required\")\n",
    "        if not self.azure_ai_search_key or not self.azure_ai_search_endpoint:\n",
    "            errors.append(\"Azure AI Search configuration is incomplete\")\n",
    "        if not self.azure_ml_workspace or not self.azure_ml_subscription_id or not self.azure_ml_resource_group:\n",
    "            errors.append(\"Azure ML configuration is incomplete\")\n",
    "\n",
    "        if errors:\n",
    "            raise ValueError(f\"Configuration errors: {', '.join(errors)}\")\n",
    "\n",
    "        print(\"‚úÖ All required Azure configurations validated successfully\")\n",
    "\n",
    "    def display_config(self):\n",
    "        \"\"\"Display current configuration (hiding secrets)\"\"\"\n",
    "        print(\"Azure OpenAI:\")\n",
    "        print(f\"  Endpoint: {self.azure_openai_endpoint}\")\n",
    "        print(f\"  Deployment: {self.azure_openai_deployment}\")\n",
    "        print(f\"  API Version: {self.azure_openai_version}\")\n",
    "        print(f\"  API Key: {'*' * 20 + self.azure_openai_key[-4:] if self.azure_openai_key else 'Not set'}\")\n",
    "\n",
    "        print(\"\\nAzure AI Search:\")\n",
    "        print(f\"  Endpoint: {self.azure_ai_search_endpoint}\")\n",
    "        print(f\"  Index: {self.azure_ai_search_index}\")\n",
    "        print(f\"  API Key: {'*' * 20 + self.azure_ai_search_key[-4:] if self.azure_ai_search_key else 'Not set'}\")\n",
    "\n",
    "        print(\"\\nAzure ML:\")\n",
    "        print(f\"  Workspace: {self.azure_ml_workspace}\")\n",
    "        print(f\"  Subscription ID: {self.azure_ml_subscription_id}\")\n",
    "        print(f\"  Resource Group: {self.azure_ml_resource_group}\")\n",
    "\n",
    "        print(\"\\nMonitoring:\")\n",
    "        print(f\"  App Insights Connection String: {'*' * 20 + self.app_insights_connection_string[-4:] if self.app_insights_connection_string else 'Not set'}\")\n",
    "        print(f\"  Environment: {self.environment}\")\n",
    "        print(f\"  Debug Mode: {self.debug}\")\n",
    "\n",
    "# Initialize configuration\n",
    "try:\n",
    "    config = NotebookConfig()\n",
    "    config.display_config()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load configuration: {e}\")\n",
    "    print(\"\\nPlease ensure your .env file includes all required Azure variables.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1183e",
   "metadata": {},
   "source": [
    "### 1.1 Basic Prompting and Model I/O with LCEL\n",
    "\n",
    "**What this does:** Demonstrates LangChain Expression Language (LCEL) for connecting prompts, models, and output parsers in a composable chain.\n",
    "\n",
    "The code below creates a basic chain that:\n",
    "- **Initializes Azure OpenAI chat model** using the configuration from above\n",
    "- **Creates a prompt template** with variables for dynamic content\n",
    "- **Uses LCEL pipe operator (`|`)** to compose prompt ‚Üí model ‚Üí output parser\n",
    "- **Parses model response** into clean string output\n",
    "\n",
    "**Key 2025 Pattern:** LCEL is LangChain's modern composition syntax that replaced legacy chains. It's similar to Unix pipes but for AI workflows.\n",
    "\n",
    "**LCEL OpenAI Chat:** This pattern works identically with Azure OpenAI - the model provider is abstracted away by LangChain's interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize Azure OpenAI with configuration\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_openai_deployment,\n",
    "    api_version=config.azure_openai_version,\n",
    "    temperature=0.7,\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    api_key=config.azure_openai_key\n",
    ") \n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Question: {question}\\nAnswer: Let's think step by step.\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca4b69",
   "metadata": {},
   "source": [
    "### 1.2 Sequential Processing with LCEL (Modern Pattern)\n",
    "\n",
    "**What this does:** Creates a complex multi-step workflow where the output of one LLM call becomes input to the next, using modern LCEL composition patterns.\n",
    "\n",
    "The code below demonstrates:\n",
    "- **Multi-step chain creation** with `RunnableParallel` for parallel execution\n",
    "- **Data passing between steps** using `RunnablePassthrough` to maintain input context\n",
    "- **Sequential workflow orchestration** where step 1 generates a company name, step 2 creates a catchphrase\n",
    "- **Modern 2025 pattern replacement** of legacy `SimpleSequentialChain` with LCEL\n",
    "\n",
    "**Key Innovation:** `RunnableParallel` allows multiple outputs in a single invocation, enabling complex multi-agent-like behaviors.\n",
    "\n",
    "**Workflow pattern Azure compliant:** This workflow pattern works identically with Azure OpenAI, and could be enhanced with Azure Application Insights for step-by-step monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38709b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# Initialize Azure OpenAI with configuration\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_openai_deployment,\n",
    "    api_version=config.azure_openai_version,\n",
    "    temperature=0.7,\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    api_key=config.azure_openai_key\n",
    ") \n",
    "\n",
    "# Modern LCEL approach - compose operations with pipe operator\n",
    "name_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is a good name for a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "catchphrase_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a creative catchphrase for the following company: {company_name}\"\n",
    ")\n",
    "\n",
    "# Build sequential chain using LCEL composition\n",
    "name_chain = name_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Create a more complex chain that passes results between steps\n",
    "def create_sequential_chain():\n",
    "    \"\"\"Creates a sequential chain using modern LCEL patterns\"\"\"\n",
    "    \n",
    "    # Step 1: Generate company name\n",
    "    step1 = (\n",
    "        {\"product\": RunnablePassthrough()} \n",
    "        | name_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Step 2: Generate catchphrase using the company name\n",
    "    step2 = (\n",
    "        {\"company_name\": step1}\n",
    "        | catchphrase_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Combine results into final output\n",
    "    return RunnableParallel({\n",
    "        \"company_name\": step1,\n",
    "        \"catchphrase\": step2,\n",
    "        \"product\": RunnablePassthrough()\n",
    "    })\n",
    "\n",
    "# Execute the sequential chain\n",
    "sequential_chain = create_sequential_chain()\n",
    "product = \"colorful, eco-friendly socks\"\n",
    "\n",
    "print(f\"Input: {product}\")\n",
    "print(\"\\nüîó Running sequential LCEL chain...\")\n",
    "\n",
    "result = sequential_chain.invoke(product)\n",
    "print(f\"\\n‚úÖ Results:\")\n",
    "print(f\"Product: {result['product']}\")\n",
    "print(f\"Company Name: {result['company_name']}\")\n",
    "print(f\"Catchphrase: {result['catchphrase']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Streaming with LCEL\n",
    "\n",
    "**What this does:** Implements real-time token streaming for improved user experience, displaying AI responses as they're generated rather than waiting for completion.\n",
    "\n",
    "The code below creates:\n",
    "- **Custom callback handler** that intercepts each token as it's generated by the LLM\n",
    "- **Streaming chain configuration** using `.with_config()` to attach the callback\n",
    "- **Real-time token display** printing each word/token immediately upon generation\n",
    "- **Production-ready streaming pattern** for chat interfaces and interactive applications\n",
    "\n",
    "**Key 2025 Feature:** Native streaming support is built into LCEL, making it effortless to add real-time responses.\n",
    "\n",
    "**Azure OpenAI streaming support:** Azure OpenAI supports streaming natively. For enterprise monitoring, combine with Azure Application Insights to track streaming performance and token usage in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e43c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "# Define a custom streaming callback handler\n",
    "class StreamingCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"Custom callback handler to demonstrate streaming\"\"\"\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "# Initialize the AzureChatOpenAI model with streaming enabled\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_openai_deployment,\n",
    "    api_version=config.azure_openai_version,\n",
    "    temperature=0.7,\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    api_key=config.azure_openai_key,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingCallbackHandler()]\n",
    ")\n",
    "\n",
    "# Create a prompt template\n",
    "streaming_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a brief story about {topic}. Make it engaging and creative.\"\n",
    ")\n",
    "\n",
    "# Build the streaming chain\n",
    "streaming_chain = (\n",
    "    streaming_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "print(\"üåä Streaming response for a story about 'space exploration':\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = streaming_chain.invoke({\"topic\": \"space exploration\"})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ufwt2kl5sol",
   "metadata": {},
   "source": [
    "### 1.4 Function Calling and Basic Agents\n",
    "\n",
    "**What this does:** Creates an intelligent agent using 2025's LangGraph framework (replacing legacy AgentExecutor) that can use tools and maintain conversation memory.\n",
    "\n",
    "The code below demonstrates:\n",
    "- **Modern 2025 LangGraph agent** using `create_react_agent` (replaces deprecated AgentExecutor)\n",
    "- **Tool integration** with Azure AI Search for enterprise search capabilities, calculator, and text formatting functions\n",
    "- **Memory persistence** using `MemorySaver` for conversation threading\n",
    "- **Streaming execution** with real-time step visibility for debugging\n",
    "- **Multi-step reasoning** where the agent plans, uses tools, and synthesizes results\n",
    "\n",
    "**Key 2025 Migration:** `create_react_agent` from LangGraph is the modern replacement for the legacy AgentExecutor pattern.\n",
    "\n",
    "**Considerations:** \n",
    "- Use **Azure Cosmos DB** instead of memory for production-grade conversation persistence\n",
    "- Monitor with **Azure Application Insights** for enterprise observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install langgraph azure-search-documents opencensus-ext-azure\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
    "import logging\n",
    "\n",
    "# Initialize Azure OpenAI\n",
    "llm_agent = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_openai_deployment,\n",
    "    api_version=config.azure_openai_version,\n",
    "    temperature=0,\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    api_key=config.azure_openai_key\n",
    ")\n",
    "\n",
    "# Setup Azure Application Insights logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(AzureLogHandler(connection_string=os.getenv(\"APPINSIGHTS_CONNECTION_STRING\")))\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"‚úÖ Azure Application Insights logging initialized.\")\n",
    "\n",
    "# Define Azure AI Search tool\n",
    "@tool\n",
    "def azure_ai_search(query: str) -> str:\n",
    "    \"\"\"Search enterprise content using Azure AI Search.\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=config.azure_ai_search_endpoint,\n",
    "            index_name=config.azure_ai_search_index,\n",
    "            credential=AzureKeyCredential(config.azure_ai_search_key)\n",
    "        )\n",
    "        results = search_client.search(query, top=2)\n",
    "        output = \"\\n\".join([doc[\"content\"] for doc in results])\n",
    "        logger.info(f\"Azure AI Search query: {query}\")\n",
    "        logger.info(f\"Azure AI Search results: {output}\")\n",
    "        return output or \"No results found.\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Azure AI Search error: {e}\")\n",
    "        return f\"Search failed: {e}\"\n",
    "\n",
    "# Define other tools\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate basic math expressions.\"\"\"\n",
    "    try:\n",
    "        allowed_chars = set('0123456789+-*/().** ')\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return \"Error: Only basic mathematical operations are allowed\"\n",
    "        logger.info(f\"Azure AI Search query: {expression}\")\n",
    "        logger.info(f\"Azure AI Search results: {allowed_chars}\")\n",
    "        return f\"Result: {eval(expression)}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Azure AI Search error: {e}\")\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "@tool \n",
    "def format_text(text: str, style: str = \"upper\") -> str:\n",
    "    \"\"\"Format text in upper, lower, or title case.\"\"\"\n",
    "    return {\n",
    "        \"upper\": text.upper(),\n",
    "        \"lower\": text.lower(),\n",
    "        \"title\": text.title()\n",
    "    }.get(style, f\"Unknown style: {style}\")\n",
    "\n",
    "# Combine tools\n",
    "tools = [azure_ai_search, calculate, format_text]\n",
    "\n",
    "# LangGraph agent setup\n",
    "system_message = \"You are a helpful assistant that can search enterprise content, calculate, and format text. Explain your reasoning step by step.\"\n",
    "memory = MemorySaver()\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm_agent,\n",
    "    tools=tools,\n",
    "    prompt=system_message,\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# Run a test query\n",
    "print(\"ü§ñ Testing Azure-centric LangGraph agent...\")\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-conversation\"}}\n",
    "query = \"Calculate 16 raised to the power of 0.5, then format the result as 'The answer is X' in title case\"\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [(\"user\", query)]},\n",
    "    config=config,\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    if step:\n",
    "        print(f\"üìù Step: {step}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ MODERN 2025 FEATURES DEMONSTRATED:\")\n",
    "print(\"   üîÑ LangGraph agent (replaces legacy AgentExecutor)\")\n",
    "print(\"   üíæ Built-in memory with conversation threading\")\n",
    "print(\"   üåä Real-time streaming execution\")\n",
    "print(\"   üîç Azure Application Insights tracing\")\n",
    "print(\"=\"* 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-integration",
   "metadata": {},
   "source": [
    "## üéØ 2025 Evaluation & Monitoring Integration\n",
    "\n",
    "**What this does:** Integrates production-grade evaluation and monitoring using the leading 2025 frameworks to ensure AI application quality and performance.\n",
    "\n",
    "The code below demonstrates:\n",
    "- **Azure Machine Learning + Prompt Flow integration** - for LLM evaluation and metrics\n",
    "- **Azure Application Insights monitoring** - production tracing and evaluation for LangChain applications\n",
    "- **Automated evaluation metrics** including Answer Relevancy, Faithfulness, and custom scoring\n",
    "- **Cost tracking and token usage** monitoring for production optimization\n",
    "- **Unit test-style evaluation** using pytest-compatible DeepEval patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025 PATTERN: Modern evaluation framework integration\n",
    "%pip install azure.ai.ml \n",
    "\n",
    "import os\n",
    "import logging\n",
    "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Setup Azure Application Insights logging\n",
    "app_insights_conn = os.getenv(\"APPINSIGHTS_CONNECTION_STRING\")\n",
    "if app_insights_conn:\n",
    "    logger = logging.getLogger(\"azure_monitoring\")\n",
    "    logger.addHandler(AzureLogHandler(connection_string=app_insights_conn))\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.info(\"‚úÖ Azure Application Insights logging enabled\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Application Insights not configured (add APPINSIGHTS_CONNECTION_STRING to enable)\")\n",
    "\n",
    "# Setup Azure ML client for Prompt Flow and experiment tracking\n",
    "try:\n",
    "    ml_client = MLClient(\n",
    "        credential=DefaultAzureCredential(),\n",
    "        subscription_id=os.getenv(\"AZURE_ML_SUBSCRIPTION_ID\"),\n",
    "        resource_group=os.getenv(\"AZURE_ML_RESOURCE_GROUP\"),\n",
    "        workspace_name=os.getenv(\"AZURE_ML_WORKSPACE\")\n",
    "    )\n",
    "    print(\"‚úÖ Azure ML client initialized\")\n",
    "    print(f\"üìä Workspace: {ml_client.workspace_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize Azure ML client: {e}\")\n",
    "    print(\"Please ensure AZURE_ML_SUBSCRIPTION_ID, AZURE_ML_RESOURCE_GROUP, and AZURE_ML_WORKSPACE are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deepeval-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025 PATTERN: Azure ML integration for comprehensive testing\n",
    "%pip install mlflow azureml-mlflow azureml azureml-core\n",
    "import os\n",
    "import mlflow\n",
    "from azure.identity import AzureCliCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Authenticate and connect to Azure ML workspace\n",
    "ml_client = MLClient(\n",
    "    credential=AzureCliCredential(),\n",
    "    subscription_id=os.getenv(\"AZURE_ML_SUBSCRIPTION_ID\"),\n",
    "    resource_group=os.getenv(\"AZURE_ML_RESOURCE_GROUP\"),\n",
    "    workspace_name=os.getenv(\"AZURE_ML_WORKSPACE\")\n",
    ")\n",
    "\n",
    "# Set MLflow tracking URI manually\n",
    "tracking_uri = os.getenv(\"AZURE_MLFLOW_TRACKING_URI\") \n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"LangGraph-Evaluation\")\n",
    "\n",
    "# Simulated evaluation logic\n",
    "input_text = \"What are the main benefits of LangGraph?\"\n",
    "actual_output = \"LangGraph provides state management, complex agent workflows, and streaming capabilities for multi-agent systems.\"\n",
    "expected_output = \"LangGraph offers state management and multi-agent orchestration capabilities.\"\n",
    "context = [\"LangGraph is LangChain's framework for building stateful, multi-agent applications\"]\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"input\", input_text)\n",
    "    mlflow.log_param(\"expected_output\", expected_output)\n",
    "    mlflow.log_param(\"actual_output\", actual_output)\n",
    "    mlflow.log_param(\"context\", str(context))\n",
    "\n",
    "    # Simulated scoring logic\n",
    "    relevancy_score = 0.85\n",
    "    faithfulness_score = 0.9\n",
    "\n",
    "    mlflow.log_metric(\"relevancy\", relevancy_score)\n",
    "    mlflow.log_metric(\"faithfulness\", faithfulness_score)\n",
    "\n",
    "    print(\"‚úÖ Evaluation logged to Azure ML via MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc330cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025 PATTERN: Cost tracking and token usage monitoring\n",
    "%pip install langchain_community\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "# Setup Azure Application Insights logging\n",
    "logger = logging.getLogger(\"azure_agent_cost_logger\")\n",
    "logger.addHandler(AzureLogHandler(connection_string=os.getenv(\"APPINSIGHTS_CONNECTION_STRING\")))\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def run_agent_with_cost_tracking(agent, query):\n",
    "    \"\"\"Run agent with comprehensive cost and performance tracking\"\"\"\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        # Configure conversation with thread ID for tracking\n",
    "        config = {\"configurable\": {\"thread_id\": \"cost-tracking-demo\"}}\n",
    "        \n",
    "        # Run the agent with streaming\n",
    "        response = None\n",
    "        for chunk in agent.stream({\"messages\": [(\"human\", query)]}, config):\n",
    "            if \"agent\" in chunk:\n",
    "                response = chunk[\"agent\"][\"messages\"][-1].content\n",
    "\n",
    "\n",
    "     # Log to Application Insights\n",
    "    logger.info(\"Agent run completed\", extra={\n",
    "        \"custom_dimensions\": {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"total_tokens\": cb.total_tokens,\n",
    "            \"prompt_tokens\": cb.prompt_tokens,\n",
    "            \"completion_tokens\": cb.completion_tokens,\n",
    "            \"estimated_cost_usd\": cb.total_cost\n",
    "        }\n",
    "    })\n",
    "   \n",
    "    # Display cost metrics\n",
    "    print(\"\\nüí∞ Cost Analysis:\")\n",
    "    print(f\"  Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"  Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"  Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"  Total Cost: ${cb.total_cost:.4f}\")\n",
    "    \n",
    "    return response, {\n",
    "        'total_tokens': cb.total_tokens,\n",
    "        'cost': cb.total_cost,\n",
    "        'prompt_tokens': cb.prompt_tokens,\n",
    "        'completion_tokens': cb.completion_tokens\n",
    "    }\n",
    "\n",
    "# Example usage with cost tracking\n",
    "if 'agent' in locals():\n",
    "    #test_query = \"What's the current weather in San Francisco?\"\n",
    "    test_query = \"What's the general climate information of San Francisco during May?\"\n",
    "    response, metrics = run_agent_with_cost_tracking(agent, test_query)\n",
    "    \n",
    "    print(f\"\\nü§ñ Agent Response: {response}\")\n",
    "    print(f\"üìä Performance Metrics: {metrics}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Agent not initialized - run the previous cells first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0eb7c8",
   "metadata": {},
   "source": [
    "## üéØ Azure Application Insights for Kusto Query\n",
    "\n",
    "**Kusto Queries:** - access the Azure Application Insights Logs and run kusto queries to retrieve metrics \n",
    "- open Azure Application Insights\n",
    "- on the left, click on Logs.\n",
    "- click on New Query\n",
    "- select KQL Mode\n",
    "\n",
    "**Run the kusto query**:\n",
    "-     traces\n",
    "-        | where customDimensions contains \"estimated_cost_usd\"\n",
    "-        | project timestamp, customDimensions.query, customDimensions.total_tokens, customDimensions.estimated_cost_usd\n",
    "-        | order by timestamp desc\n",
    "\n",
    "**Result**:\n",
    "- timestamp [UTC] = 2025-09-16T23:56:09.550352Z\n",
    "- customDimensions_query = What's the general climate information of San Francisco during May?\n",
    "- customDimensions_total_tokens = 502\n",
    "- customDimensions_estimated_cost_usd = 0.0022040000000000002\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Reference: Kusto Query Language overview**:\n",
    "- https://learn.microsoft.com/en-us/kusto/query/?view=microsoft-fabric\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4e482",
   "metadata": {},
   "source": [
    "## üéØ Reference:\n",
    "\n",
    "\n",
    "\n",
    "**Microsoft Azure Monitor Opentelemetry Exporter Trace Python Samples**\n",
    "- https://learn.microsoft.com/en-us/samples/azure/azure-sdk-for-python/microsoft-azure-monitor-opentelemetry-exporter-trace-python-samples/\n",
    "\n",
    "**Monitor OpenAI Agents SDK with Application Insights:**:\n",
    "- https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/monitor-openai-agents-sdk-with-application-insights/4393949\n",
    "\n",
    "**Monitor Azure OpenAI**:\n",
    "- https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/monitor-openai\n",
    "\n",
    "**Integrating Azure Application Insights using opentelemetry-instrumentation in the Sample-app-aoai-chatGPT**:\n",
    "- https://github.com/microsoft/sample-app-aoai-chatGPT/issues/495\n",
    "\n",
    "**Visualize traces on Azure AI Foundry Tracing UI**:\n",
    "- https://learn.microsoft.com/en-us/semantic-kernel/concepts/enterprise-readiness/observability/telemetry-with-azure-ai-foundry-tracing\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y3bla5ukyb9",
   "metadata": {},
   "source": [
    "## Summary: LangChain Fundamentals (2025 Edition)\n",
    "\n",
    "This notebook covered the core concepts of LangChain using modern patterns:\n",
    "\n",
    "### Key Concepts Learned:\n",
    "1. **LCEL (LangChain Expression Language)**: Modern composition using pipe operators\n",
    "2. **Sequential Processing**: Chaining operations with result passing between steps\n",
    "3. **Streaming**: Real-time token streaming for better user experience  \n",
    "4. **LangGraph Agents**: Modern agent framework replacing legacy AgentExecutor\n",
    "5. **Evaluation Integration**: Azure Apllication Insights monitoring and Azure ML + Prompt Flow for testing frameworks\n",
    "6. **Cost Tracking**: Production-ready token usage and cost monitoring\n",
    "\n",
    "### 2025 LangChain Evolution:\n",
    "- **LangGraph**: State management and multi-agent orchestration\n",
    "- **Modern Agent Patterns**: Memory-enabled agents with conversation threading\n",
    "- **Evaluation Frameworks**: Comprehensive testing with Azure ML integration\n",
    "- **Production Monitoring**: Built-in cost tracking and performance metrics\n",
    "\n",
    "### 2025 Production Considerations:\n",
    "- Enable Azure Application Insights for tracing for production monitoring\n",
    "- Implement Azure ML for comprehensive agent testing\n",
    "- Use conversation threading for memory-enabled applications\n",
    "- Monitor costs with built-in callback handlers\n",
    "- Leverage LangGraph for complex multi-agent orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6a4de",
   "metadata": {},
   "source": [
    "## üéØ Appendix\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "An example to show an application using Opentelemetry tracing api and sdk. \n",
    "Custom dependencies are:\n",
    "- tracked via spans \n",
    "- telemetry is exported to application insights with the AzureMonitorTraceExporter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49caa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "%pip install openinference-instrumentation-langchain opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp-proto-http\n",
    "\n",
    "# mypy: disable-error-code=\"attr-defined\"\n",
    "import os\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()#'azure.env')\n",
    "\n",
    "exporter = AzureMonitorTraceExporter.from_connection_string(\n",
    "    os.getenv(\"APPINSIGHTS_CONNECTION_STRING\")\n",
    ")\n",
    "\n",
    "tracer_provider = TracerProvider()\n",
    "\n",
    "trace_api.set_tracer_provider(tracer_provider)\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "tracer = trace.get_tracer(__name__)\n",
    "span_processor = BatchSpanProcessor(exporter, schedule_delay_millis=60000)\n",
    "trace.get_tracer_provider().add_span_processor(span_processor)\n",
    "LangChainInstrumentor().instrument()\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "prompt = PromptTemplate(input_variables=[\"adjective\"], template=prompt_template)\n",
    "llm = AzureChatOpenAI(api_key = os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "                      azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'), \n",
    "                      api_version = '2024-06-01', \n",
    "                      model= os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME'))\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, metadata={\"category\": \"jokes\"})\n",
    "completion = chain.predict(adjective=\"funny\", metadata={\"variant\": \"funny\"})\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6358cc",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
