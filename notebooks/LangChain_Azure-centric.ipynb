{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On with LangChain\n",
    "\n",
    "This notebook demonstrates LangChain's **2025 capabilities** using Azure OpenAI. We'll explore modern LCEL patterns, LangGraph agents (replacing legacy AgentExecutor), Azure Application Insights  integration, and evaluation frameworks.\n",
    "\n",
    "## üÜï What's New in 2025:\n",
    "- **LangGraph** replaces legacy AgentExecutor patterns\n",
    "- **Azure Application Insights** production-ready monitoring and evaluation  \n",
    "- **LangChain Sandbox** for safe code execution\n",
    "- **Modern evaluation frameworks** integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "**What this does:** Securely loads API keys and configuration from a `.env` file and validates all required settings for the 2025 LangChain stack.\n",
    "\n",
    "The code below creates a comprehensive configuration manager that:\n",
    "- **Loads environment variables** using `python-dotenv` (industry standard for secure credential management)\n",
    "- **Validates Azure OpenAI credentials** (API key, endpoint, deployment name)\n",
    "- **Configures Azure Application Insights + OpenTelemetry for monitoring** for production observability\n",
    "- **Sets up AI Search** for web search capabilities\n",
    "- **Enables 2025 evaluation frameworks** like Azure ML and MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286ac5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded environment from: c:\\Projects\\Siemens-Energy\\langchain-azure-centric-main - wip\\notebooks\\.env\n",
      "‚úÖ All required Azure configurations validated successfully\n",
      "Azure OpenAI:\n",
      "  Endpoint: https://oai-pcm-001.openai.azure.com\n",
      "  Deployment: chat-deployment\n",
      "  API Version: 2023-05-15\n",
      "  API Key: ********************iIRs\n",
      "\n",
      "Azure AI Search:\n",
      "  Endpoint: https://ais-pcm-001.search.windows.net\n",
      "  Index: documents-index\n",
      "  API Key: ********************mGL0\n",
      "\n",
      "Azure ML:\n",
      "  Workspace: ml-workspace-pcm\n",
      "  Subscription ID: \"b0e6535c-d468-4bf0-81c2-4812406b7754\"  # already present\n",
      "  Resource Group: rg-sie-energy-pcm-workshop-001\n",
      "\n",
      "Monitoring:\n",
      "  App Insights Connection String: ********************cee6\n",
      "  Environment: development\n",
      "  Debug Mode: False\n"
     ]
    }
   ],
   "source": [
    "# Environment and configuration setup with 2025 LangSmith integration\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "class NotebookConfig:\n",
    "    \"\"\"Azure-centric configuration management for Jupyter notebooks\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        env_path = find_dotenv()\n",
    "        if env_path:\n",
    "            load_dotenv(env_path)\n",
    "            print(f\"‚úÖ Loaded environment from: {env_path}\")\n",
    "        else:\n",
    "            warnings.warn(\"No .env file found. Using system environment variables only.\")\n",
    "        \n",
    "        self._load_azure_config()\n",
    "        self._load_monitoring_config()\n",
    "        self._validate_config()\n",
    "\n",
    "    def _load_azure_config(self):\n",
    "        \"\"\"Load Azure service configurations\"\"\"\n",
    "        self.azure_openai_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "        self.azure_openai_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "        self.azure_openai_deployment = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME', 'gpt-4o-mini')\n",
    "        self.azure_openai_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-05-01-preview')\n",
    "\n",
    "        self.azure_ai_search_key = os.getenv('AZURE_AI_SEARCH_KEY')\n",
    "        self.azure_ai_search_endpoint = os.getenv('AZURE_AI_SEARCH_ENDPOINT')\n",
    "        self.azure_ai_search_index = os.getenv('AZURE_AI_SEARCH_INDEX')\n",
    "\n",
    "        self.azure_ml_workspace = os.getenv('AZURE_ML_WORKSPACE')\n",
    "        self.azure_ml_subscription_id = os.getenv('AZURE_ML_SUBSCRIPTION_ID')\n",
    "        self.azure_ml_resource_group = os.getenv('AZURE_ML_RESOURCE_GROUP')\n",
    "\n",
    "    def _load_monitoring_config(self):\n",
    "        \"\"\"Load Azure Application Insights configuration\"\"\"\n",
    "        self.app_insights_connection_string = os.getenv('APPINSIGHTS_CONNECTION_STRING')\n",
    "        self.environment = os.getenv('ENVIRONMENT', 'development')\n",
    "        self.debug = os.getenv('DEBUG', 'false').lower() == 'true'\n",
    "\n",
    "    def _validate_config(self):\n",
    "        \"\"\"Validate required Azure configurations\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        if not self.azure_openai_key:\n",
    "            errors.append(\"AZURE_OPENAI_API_KEY is required\")\n",
    "        if not self.azure_openai_endpoint:\n",
    "            errors.append(\"AZURE_OPENAI_ENDPOINT is required\")\n",
    "        if not self.azure_ai_search_key or not self.azure_ai_search_endpoint:\n",
    "            errors.append(\"Azure AI Search configuration is incomplete\")\n",
    "        if not self.azure_ml_workspace or not self.azure_ml_subscription_id or not self.azure_ml_resource_group:\n",
    "            errors.append(\"Azure ML configuration is incomplete\")\n",
    "\n",
    "        if errors:\n",
    "            raise ValueError(f\"Configuration errors: {', '.join(errors)}\")\n",
    "\n",
    "        print(\"‚úÖ All required Azure configurations validated successfully\")\n",
    "\n",
    "    def display_config(self):\n",
    "        \"\"\"Display current configuration (hiding secrets)\"\"\"\n",
    "        print(\"Azure OpenAI:\")\n",
    "        print(f\"  Endpoint: {self.azure_openai_endpoint}\")\n",
    "        print(f\"  Deployment: {self.azure_openai_deployment}\")\n",
    "        print(f\"  API Version: {self.azure_openai_version}\")\n",
    "        print(f\"  API Key: {'*' * 20 + self.azure_openai_key[-4:] if self.azure_openai_key else 'Not set'}\")\n",
    "\n",
    "        print(\"\\nAzure AI Search:\")\n",
    "        print(f\"  Endpoint: {self.azure_ai_search_endpoint}\")\n",
    "        print(f\"  Index: {self.azure_ai_search_index}\")\n",
    "        print(f\"  API Key: {'*' * 20 + self.azure_ai_search_key[-4:] if self.azure_ai_search_key else 'Not set'}\")\n",
    "\n",
    "        print(\"\\nAzure ML:\")\n",
    "        print(f\"  Workspace: {self.azure_ml_workspace}\")\n",
    "        print(f\"  Subscription ID: {self.azure_ml_subscription_id}\")\n",
    "        print(f\"  Resource Group: {self.azure_ml_resource_group}\")\n",
    "\n",
    "        print(\"\\nMonitoring:\")\n",
    "        print(f\"  App Insights Connection String: {'*' * 20 + self.app_insights_connection_string[-4:] if self.app_insights_connection_string else 'Not set'}\")\n",
    "        print(f\"  Environment: {self.environment}\")\n",
    "        print(f\"  Debug Mode: {self.debug}\")\n",
    "\n",
    "# Initialize configuration\n",
    "try:\n",
    "    config = NotebookConfig()\n",
    "    config.display_config()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load configuration: {e}\")\n",
    "    print(\"\\nPlease ensure your .env file includes all required Azure variables.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1183e",
   "metadata": {},
   "source": [
    "### 1.1 Basic Prompting and Model I/O with LCEL\n",
    "\n",
    "**What this does:** Demonstrates LangChain Expression Language (LCEL) for connecting prompts, models, and output parsers in a composable chain.\n",
    "\n",
    "The code below creates a basic chain that:\n",
    "- **Initializes Azure OpenAI chat model** using the configuration from above\n",
    "- **Creates a prompt template** with variables for dynamic content\n",
    "- **Uses LCEL pipe operator (`|`)** to compose prompt ‚Üí model ‚Üí output parser\n",
    "- **Parses model response** into clean string output\n",
    "\n",
    "**Key 2025 Pattern:** LCEL is LangChain's modern composition syntax that replaced legacy chains. It's similar to Unix pipes but for AI workflows.\n",
    "\n",
    "**LCEL OpenAI Chat:** This pattern works identically with Azure OpenAI - the model provider is abstracted away by LangChain's interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbe9ff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Sure! Let's break it down step by step:\n",
      "\n",
      "1. France is a country located in Western Europe.\n",
      "2. Every country has a main city known as its \"capital,\" where the government is usually based.\n",
      "3. The capital of France is well-known for its history, culture, and landmarks like the Eiffel Tower.\n",
      "4. The name of this city is Paris.\n",
      "\n",
      "**Final Answer:** The capital of France is Paris.\n",
      "Sure! Let's break it down step by step:\n",
      "\n",
      "1. France is a country located in Western Europe.\n",
      "2. Every country has a main city known as its \"capital,\" where the government is usually based.\n",
      "3. The capital of France is well-known for its history, culture, and landmarks like the Eiffel Tower.\n",
      "4. The name of this city is Paris.\n",
      "\n",
      "**Final Answer:** The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet langchain_openai\n",
    "%pip install --quiet pydantic pydantic-core\n",
    "\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize Azure OpenAI with configuration\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_openai_deployment,\n",
    "    api_version=config.azure_openai_version,\n",
    "    temperature=0.7,\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    api_key=config.azure_openai_key\n",
    ") \n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Question: {question}\\nAnswer: Let's think step by step.\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\": \"What is the capital of France?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca4b69",
   "metadata": {},
   "source": [
    "### 1.2 Sequential Processing with LCEL (Modern Pattern)\n",
    "\n",
    "**What this does:** Creates a complex multi-step workflow where the output of one LLM call becomes input to the next, using modern LCEL composition patterns.\n",
    "\n",
    "The code below demonstrates:\n",
    "- **Multi-step chain creation** with `RunnableParallel` for parallel execution\n",
    "- **Data passing between steps** using `RunnablePassthrough` to maintain input context\n",
    "- **Sequential workflow orchestration** where step 1 generates a company name, step 2 creates a catchphrase\n",
    "- **Modern 2025 pattern replacement** of legacy `SimpleSequentialChain` with LCEL\n",
    "\n",
    "**Key Innovation:** `RunnableParallel` allows multiple outputs in a single invocation, enabling complex multi-agent-like behaviors.\n",
    "\n",
    "**Workflow pattern Azure compliant:** This workflow pattern works identically with Azure OpenAI, and could be enhanced with Azure Application Insights for step-by-step monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38709b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: colorful, eco-friendly socks\n",
      "\n",
      "üîó Running sequential LCEL chain...\n",
      "\n",
      "‚úÖ Results:\n",
      "Product: colorful, eco-friendly socks\n",
      "Company Name: Absolutely! Here are some name ideas for a company that makes colorful, eco-friendly socks:\n",
      "\n",
      "1. **EcoSole Socks**\n",
      "2. **VividStep**\n",
      "3. **GreenToes**\n",
      "4. **Colorleaf Socks**\n",
      "5. **SockSprout**\n",
      "6. **EcoChroma**\n",
      "7. **BrightRoots Socks**\n",
      "8. **SoleBloom**\n",
      "9. **EarthHue Socks**\n",
      "10. **HappyEarth Socks**\n",
      "11. **RainbowRoots**\n",
      "12. **EcoJive Socks**\n",
      "13. **Sprout & Stitch**\n",
      "14. **LeafyLoops**\n",
      "15. **PureHue Socks**\n",
      "\n",
      "Let me know if you‚Äôd like names in a particular style or with certain words included!\n",
      "Catchphrase: Absolutely! Here are some creative catchphrases for a colorful, eco-friendly sock company (using your name ideas):\n",
      "\n",
      "---\n",
      "\n",
      "**1. EcoSole Socks**  \n",
      "*‚ÄúStep Bright. Tread Light.‚Äù*\n",
      "\n",
      "**2. GreenStep Socks**  \n",
      "*‚ÄúEvery Step, a Greener Tomorrow.‚Äù*\n",
      "\n",
      "**3. ChromaSole**  \n",
      "*‚ÄúColor Your World, Conserve Your Earth.‚Äù*\n",
      "\n",
      "**4. VividLeaf Socks**  \n",
      "*‚ÄúVivid Color. Verdant Conscience.‚Äù*\n",
      "\n",
      "**5. SockSprout**  \n",
      "*‚ÄúWhere Style Grows Sustainably.‚Äù*\n",
      "\n",
      "**6. BrightRoots Socks**  \n",
      "*‚ÄúGrounded in Color, Rooted in Care.‚Äù*\n",
      "\n",
      "**7. EcoHues**  \n",
      "*‚ÄúWear the Rainbow. Save the Planet.‚Äù*\n",
      "\n",
      "**8. ColorKind Socks**  \n",
      "*‚ÄúKind to Feet. Kinder to Earth.‚Äù*\n",
      "\n",
      "**9. SustainSocks**  \n",
      "*‚ÄúSocks for Today. Sustaining Tomorrow.‚Äù*\n",
      "\n",
      "**10. PurePlay Socks**  \n",
      "*‚ÄúPure Comfort. Playful Colors. Positive Impact.‚Äù*\n",
      "\n",
      "**11. ColorCycle Socks**  \n",
      "*‚ÄúBright Steps, Better Planet.‚Äù*\n",
      "\n",
      "**12. Happy Earth Socks**  \n",
      "*‚ÄúHappy Feet, Happier Earth.‚Äù*\n",
      "\n",
      "**13. SoleBloom**  \n",
      "*‚ÄúLet Your Style Blossom‚ÄîNaturally.‚Äù*\n",
      "\n",
      "**14. EarthJoy Socks**  \n",
      "*‚ÄúJoyful Colors. Joyful Planet.‚Äù*\n",
      "\n",
      "**15. VivaVerde Socks**  \n",
      "*‚ÄúLive Colorfully. Tread Green.‚Äù*\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you‚Äôd like these with a more playful, modern, or elegant twist!\n",
      "\n",
      "‚úÖ Results:\n",
      "Product: colorful, eco-friendly socks\n",
      "Company Name: Absolutely! Here are some name ideas for a company that makes colorful, eco-friendly socks:\n",
      "\n",
      "1. **EcoSole Socks**\n",
      "2. **VividStep**\n",
      "3. **GreenToes**\n",
      "4. **Colorleaf Socks**\n",
      "5. **SockSprout**\n",
      "6. **EcoChroma**\n",
      "7. **BrightRoots Socks**\n",
      "8. **SoleBloom**\n",
      "9. **EarthHue Socks**\n",
      "10. **HappyEarth Socks**\n",
      "11. **RainbowRoots**\n",
      "12. **EcoJive Socks**\n",
      "13. **Sprout & Stitch**\n",
      "14. **LeafyLoops**\n",
      "15. **PureHue Socks**\n",
      "\n",
      "Let me know if you‚Äôd like names in a particular style or with certain words included!\n",
      "Catchphrase: Absolutely! Here are some creative catchphrases for a colorful, eco-friendly sock company (using your name ideas):\n",
      "\n",
      "---\n",
      "\n",
      "**1. EcoSole Socks**  \n",
      "*‚ÄúStep Bright. Tread Light.‚Äù*\n",
      "\n",
      "**2. GreenStep Socks**  \n",
      "*‚ÄúEvery Step, a Greener Tomorrow.‚Äù*\n",
      "\n",
      "**3. ChromaSole**  \n",
      "*‚ÄúColor Your World, Conserve Your Earth.‚Äù*\n",
      "\n",
      "**4. VividLeaf Socks**  \n",
      "*‚ÄúVivid Color. Verdant Conscience.‚Äù*\n",
      "\n",
      "**5. SockSprout**  \n",
      "*‚ÄúWhere Style Grows Sustainably.‚Äù*\n",
      "\n",
      "**6. BrightRoots Socks**  \n",
      "*‚ÄúGrounded in Color, Rooted in Care.‚Äù*\n",
      "\n",
      "**7. EcoHues**  \n",
      "*‚ÄúWear the Rainbow. Save the Planet.‚Äù*\n",
      "\n",
      "**8. ColorKind Socks**  \n",
      "*‚ÄúKind to Feet. Kinder to Earth.‚Äù*\n",
      "\n",
      "**9. SustainSocks**  \n",
      "*‚ÄúSocks for Today. Sustaining Tomorrow.‚Äù*\n",
      "\n",
      "**10. PurePlay Socks**  \n",
      "*‚ÄúPure Comfort. Playful Colors. Positive Impact.‚Äù*\n",
      "\n",
      "**11. ColorCycle Socks**  \n",
      "*‚ÄúBright Steps, Better Planet.‚Äù*\n",
      "\n",
      "**12. Happy Earth Socks**  \n",
      "*‚ÄúHappy Feet, Happier Earth.‚Äù*\n",
      "\n",
      "**13. SoleBloom**  \n",
      "*‚ÄúLet Your Style Blossom‚ÄîNaturally.‚Äù*\n",
      "\n",
      "**14. EarthJoy Socks**  \n",
      "*‚ÄúJoyful Colors. Joyful Planet.‚Äù*\n",
      "\n",
      "**15. VivaVerde Socks**  \n",
      "*‚ÄúLive Colorfully. Tread Green.‚Äù*\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you‚Äôd like these with a more playful, modern, or elegant twist!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# Initialize Azure OpenAI with configuration\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_openai_deployment,\n",
    "    api_version=config.azure_openai_version,\n",
    "    temperature=0.7,\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    api_key=config.azure_openai_key\n",
    ") \n",
    "\n",
    "# Modern LCEL approach - compose operations with pipe operator\n",
    "name_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is a good name for a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "catchphrase_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a creative catchphrase for the following company: {company_name}\"\n",
    ")\n",
    "\n",
    "# Build sequential chain using LCEL composition\n",
    "name_chain = name_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Create a more complex chain that passes results between steps\n",
    "def create_sequential_chain():\n",
    "    \"\"\"Creates a sequential chain using modern LCEL patterns\"\"\"\n",
    "    \n",
    "    # Step 1: Generate company name\n",
    "    step1 = (\n",
    "        {\"product\": RunnablePassthrough()} \n",
    "        | name_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Step 2: Generate catchphrase using the company name\n",
    "    step2 = (\n",
    "        {\"company_name\": step1}\n",
    "        | catchphrase_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Combine results into final output\n",
    "    return RunnableParallel({\n",
    "        \"company_name\": step1,\n",
    "        \"catchphrase\": step2,\n",
    "        \"product\": RunnablePassthrough()\n",
    "    })\n",
    "\n",
    "# Execute the sequential chain\n",
    "sequential_chain = create_sequential_chain()\n",
    "product = \"colorful, eco-friendly socks\"\n",
    "\n",
    "print(f\"Input: {product}\")\n",
    "print(\"\\nüîó Running sequential LCEL chain...\")\n",
    "\n",
    "result = sequential_chain.invoke(product)\n",
    "print(f\"\\n‚úÖ Results:\")\n",
    "print(f\"Product: {result['product']}\")\n",
    "print(f\"Company Name: {result['company_name']}\")\n",
    "print(f\"Catchphrase: {result['catchphrase']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c8714",
   "metadata": {},
   "source": [
    "### 1.3 Streaming with LCEL\n",
    "\n",
    "**What this does:** Implements real-time token streaming for improved user experience, displaying AI responses as they're generated rather than waiting for completion.\n",
    "\n",
    "The code below creates:\n",
    "- **Custom callback handler** that intercepts each token as it's generated by the LLM\n",
    "- **Streaming chain configuration** using `.with_config()` to attach the callback\n",
    "- **Real-time token display** printing each word/token immediately upon generation\n",
    "- **Production-ready streaming pattern** for chat interfaces and interactive applications\n",
    "\n",
    "**Key 2025 Feature:** Native streaming support is built into LCEL, making it effortless to add real-time responses.\n",
    "\n",
    "**Azure OpenAI streaming support:** Azure OpenAI supports streaming natively. For enterprise monitoring, combine with Azure Application Insights to track streaming performance and token usage in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71e43c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåä Streaming response for a story about 'space exploration':\n",
      "==================================================\n",
      "Captain Mira Chen‚Äôs boots echoed through the humming corridorCaptain Mira Chen‚Äôs boots echoed through the humming corridor of of the starship *Odyssey*. Beyond the the starship *Odyssey*. Beyond the reinforced windows reinforced windows, Saturn‚Äôs rings glimmered like cosmic, Saturn‚Äôs rings glimmered like cosmic jewelry, silent and jewelry, silent and eternal.\n",
      "\n",
      "Her mission was simple eternal.\n",
      "\n",
      "Her mission was simple, yet daunting: chart a route through, yet daunting: chart a route through the Perseid Nebula, an uncharted swirl the Perseid Nebula, an uncharted swirl of stardust rumored to of stardust rumored to hide new worlds. The nebula‚Äôs colors hide new worlds. The nebula‚Äôs colors danced danced across the hull‚Äîviolet, emerald, across the hull‚Äîviolet, emerald, and and gold‚Äîcasting prismatic shadows.\n",
      "\n",
      " gold‚Äîcasting prismatic shadows.\n",
      "\n",
      "As theAs the ship plunged into the nebula ship plunged into the nebula, the sensors, the sensors flickered. Mira flickered. Mira‚Äôs heart pounded‚Äôs heart pounded. Suddenly, a. Suddenly, a transmission‚Äîstrange transmission‚Äîstrange, melodic pulses‚Äîfilled, melodic pulses‚Äîfilled the bridge. the bridge. The patterns weren‚Äôt random. The patterns weren‚Äôt random. They were They were music, repeating music, repeating like an invitation.\n",
      "\n",
      "‚Äú like an invitation.\n",
      "\n",
      "‚ÄúTranslate,‚Äù Mira whispered. The computerTranslate,‚Äù Mira whispered. The computer worked, and soon a shimmering holog worked, and soon a shimmering hologram appeared: a map, leading to a planetram appeared: a map, leading to a planet nestled deep within the nebula nestled deep within the nebula‚Äôs heart.\n",
      "\n",
      "With trembling hands, Mira‚Äôs heart.\n",
      "\n",
      "With trembling hands, Mira set course. Humanity wasn‚Äôt alone; the set course. Humanity wasn‚Äôt alone; the universe was reaching out, not with words, universe was reaching out, not with words, but with song. but with song. And Mira And Mira, awestruck, awestruck, was, was the first to listen. the first to listen.\n",
      "==================================================\n",
      "‚úÖ Streaming complete!\n",
      "\n",
      "==================================================\n",
      "‚úÖ Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "# Define a custom streaming callback handler\n",
    "class StreamingCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"Custom callback handler to demonstrate streaming\"\"\"\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "# Initialize the AzureChatOpenAI model with streaming enabled\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_openai_deployment,\n",
    "    api_version=config.azure_openai_version,\n",
    "    temperature=0.7,\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    api_key=config.azure_openai_key,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingCallbackHandler()]\n",
    ")\n",
    "\n",
    "# Create a prompt template\n",
    "streaming_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a brief story about {topic}. Make it engaging and creative.\"\n",
    ")\n",
    "\n",
    "# Build the streaming chain\n",
    "streaming_chain = (\n",
    "    streaming_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "print(\"üåä Streaming response for a story about 'space exploration':\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response = streaming_chain.invoke({\"topic\": \"space exploration\"})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ufwt2kl5sol",
   "metadata": {},
   "source": [
    "### 1.4 Function Calling and Basic Agents\n",
    "\n",
    "**What this does:** Creates an intelligent agent using 2025's LangGraph framework (replacing legacy AgentExecutor) that can use tools and maintain conversation memory.\n",
    "\n",
    "The code below demonstrates:\n",
    "- **Modern 2025 LangGraph agent** using `create_react_agent` (replaces deprecated AgentExecutor)\n",
    "- **Tool integration** with Azure AI Search for enterprise search capabilities, calculator, and text formatting functions\n",
    "- **Memory persistence** using `MemorySaver` for conversation threading\n",
    "- **Streaming execution** with real-time step visibility for debugging\n",
    "- **Multi-step reasoning** where the agent plans, uses tools, and synthesizes results\n",
    "\n",
    "**Key 2025 Migration:** `create_react_agent` from LangGraph is the modern replacement for the legacy AgentExecutor pattern.\n",
    "\n",
    "**Considerations:** \n",
    "- Use **Azure Cosmos DB** instead of memory for production-grade conversation persistence\n",
    "- Monitor with **Azure Application Insights** for enterprise observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90b6d2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ü§ñ Testing Azure-centric LangGraph agent...\n",
      "üìù Step: {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Q4w4cbBOEUSqyWLmEQDJMgHg', 'function': {'arguments': '{\"expression\": \"16^0.5\"}', 'name': 'calculate'}, 'type': 'function'}, {'id': 'call_XuGrtqtn5yn7sH2VhYype9r4', 'function': {'arguments': '{\"text\": \"The answer is X\", \"style\": \"title\"}', 'name': 'format_text'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 151, 'total_tokens': 206, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_9ab7d013ff', 'id': 'chatcmpl-CIpki56dUNubb4PqaUMhFimddfazp', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run--c9d3847b-03e9-4a4e-a3c1-bee1cc57eb12-0', tool_calls=[{'name': 'calculate', 'args': {'expression': '16^0.5'}, 'id': 'call_Q4w4cbBOEUSqyWLmEQDJMgHg', 'type': 'tool_call'}, {'name': 'format_text', 'args': {'text': 'The answer is X', 'style': 'title'}, 'id': 'call_XuGrtqtn5yn7sH2VhYype9r4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 55, 'total_tokens': 206, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "üìù Step: {'tools': {'messages': [ToolMessage(content='Error: Only basic mathematical operations are allowed', name='calculate', tool_call_id='call_Q4w4cbBOEUSqyWLmEQDJMgHg')]}}\n",
      "üìù Step: {'tools': {'messages': [ToolMessage(content='The Answer Is X', name='format_text', tool_call_id='call_XuGrtqtn5yn7sH2VhYype9r4')]}}\n",
      "üìù Step: {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Q4w4cbBOEUSqyWLmEQDJMgHg', 'function': {'arguments': '{\"expression\": \"16^0.5\"}', 'name': 'calculate'}, 'type': 'function'}, {'id': 'call_XuGrtqtn5yn7sH2VhYype9r4', 'function': {'arguments': '{\"text\": \"The answer is X\", \"style\": \"title\"}', 'name': 'format_text'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 151, 'total_tokens': 206, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_9ab7d013ff', 'id': 'chatcmpl-CIpki56dUNubb4PqaUMhFimddfazp', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run--c9d3847b-03e9-4a4e-a3c1-bee1cc57eb12-0', tool_calls=[{'name': 'calculate', 'args': {'expression': '16^0.5'}, 'id': 'call_Q4w4cbBOEUSqyWLmEQDJMgHg', 'type': 'tool_call'}, {'name': 'format_text', 'args': {'text': 'The answer is X', 'style': 'title'}, 'id': 'call_XuGrtqtn5yn7sH2VhYype9r4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 55, 'total_tokens': 206, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "üìù Step: {'tools': {'messages': [ToolMessage(content='Error: Only basic mathematical operations are allowed', name='calculate', tool_call_id='call_Q4w4cbBOEUSqyWLmEQDJMgHg')]}}\n",
      "üìù Step: {'tools': {'messages': [ToolMessage(content='The Answer Is X', name='format_text', tool_call_id='call_XuGrtqtn5yn7sH2VhYype9r4')]}}\n",
      "üìù Step: {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_K4hTyrhWUsJFuv0SsAJ9bU2M', 'function': {'arguments': '{\"expression\":\"16^(1/2)\"}', 'name': 'calculate'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 231, 'total_tokens': 250, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_9ab7d013ff', 'id': 'chatcmpl-CIpkj26HE9yXaNe48RnCeCd3Kgrzu', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run--0d451ab8-d1d0-4ebc-92dc-4583af623e7e-0', tool_calls=[{'name': 'calculate', 'args': {'expression': '16^(1/2)'}, 'id': 'call_K4hTyrhWUsJFuv0SsAJ9bU2M', 'type': 'tool_call'}], usage_metadata={'input_tokens': 231, 'output_tokens': 19, 'total_tokens': 250, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "üìù Step: {'tools': {'messages': [ToolMessage(content='Error: Only basic mathematical operations are allowed', name='calculate', id='48c3a90c-9a80-43cb-a47b-313d8e10204e', tool_call_id='call_K4hTyrhWUsJFuv0SsAJ9bU2M')]}}\n",
      "üìù Step: {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_K4hTyrhWUsJFuv0SsAJ9bU2M', 'function': {'arguments': '{\"expression\":\"16^(1/2)\"}', 'name': 'calculate'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 231, 'total_tokens': 250, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_9ab7d013ff', 'id': 'chatcmpl-CIpkj26HE9yXaNe48RnCeCd3Kgrzu', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run--0d451ab8-d1d0-4ebc-92dc-4583af623e7e-0', tool_calls=[{'name': 'calculate', 'args': {'expression': '16^(1/2)'}, 'id': 'call_K4hTyrhWUsJFuv0SsAJ9bU2M', 'type': 'tool_call'}], usage_metadata={'input_tokens': 231, 'output_tokens': 19, 'total_tokens': 250, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "üìù Step: {'tools': {'messages': [ToolMessage(content='Error: Only basic mathematical operations are allowed', name='calculate', id='48c3a90c-9a80-43cb-a47b-313d8e10204e', tool_call_id='call_K4hTyrhWUsJFuv0SsAJ9bU2M')]}}\n",
      "üìù Step: {'agent': {'messages': [AIMessage(content='It appears that only basic mathematical operations (addition, subtraction, multiplication, division) are allowed, and exponentiation is not supported directly.\\n\\nHowever, 16 raised to the power of 0.5 is the same as the square root of 16, which is 4.\\n\\nSo, formatted in title case: The Answer Is 4.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 264, 'total_tokens': 335, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_9ab7d013ff', 'id': 'chatcmpl-CIpkk69zZbbXr2SqeQsFIVbVzFBYe', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run--b3e76142-bb7b-4cda-81f3-793f60046cc8-0', usage_metadata={'input_tokens': 264, 'output_tokens': 71, 'total_tokens': 335, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\n",
      "==================================================\n",
      "‚úÖ MODERN 2025 FEATURES DEMONSTRATED:\n",
      "   üîÑ LangGraph agent (replaces legacy AgentExecutor)\n",
      "   üíæ Built-in memory with conversation threading\n",
      "   üåä Real-time streaming execution\n",
      "   üîç Azure Application Insights tracing\n",
      "==================================================\n",
      "üìù Step: {'agent': {'messages': [AIMessage(content='It appears that only basic mathematical operations (addition, subtraction, multiplication, division) are allowed, and exponentiation is not supported directly.\\n\\nHowever, 16 raised to the power of 0.5 is the same as the square root of 16, which is 4.\\n\\nSo, formatted in title case: The Answer Is 4.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 264, 'total_tokens': 335, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_9ab7d013ff', 'id': 'chatcmpl-CIpkk69zZbbXr2SqeQsFIVbVzFBYe', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run--b3e76142-bb7b-4cda-81f3-793f60046cc8-0', usage_metadata={'input_tokens': 264, 'output_tokens': 71, 'total_tokens': 335, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\n",
      "==================================================\n",
      "‚úÖ MODERN 2025 FEATURES DEMONSTRATED:\n",
      "   üîÑ LangGraph agent (replaces legacy AgentExecutor)\n",
      "   üíæ Built-in memory with conversation threading\n",
      "   üåä Real-time streaming execution\n",
      "   üîç Azure Application Insights tracing\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install --quiet langgraph azure-search-documents opencensus-ext-azure\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
    "import logging\n",
    "\n",
    "# Initialize Azure OpenAI\n",
    "llm_agent = AzureChatOpenAI(\n",
    "    azure_deployment=config.azure_openai_deployment,\n",
    "    api_version=config.azure_openai_version,\n",
    "    temperature=0,\n",
    "    azure_endpoint=config.azure_openai_endpoint,\n",
    "    api_key=config.azure_openai_key\n",
    ")\n",
    "\n",
    "# Setup Azure Application Insights logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(AzureLogHandler(connection_string=os.getenv(\"APPINSIGHTS_CONNECTION_STRING\")))\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"‚úÖ Azure Application Insights logging initialized.\")\n",
    "\n",
    "# Define Azure AI Search tool\n",
    "@tool\n",
    "def azure_ai_search(query: str) -> str:\n",
    "    \"\"\"Search enterprise content using Azure AI Search.\"\"\"\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "            endpoint=config.azure_ai_search_endpoint,\n",
    "            index_name=config.azure_ai_search_index,\n",
    "            credential=AzureKeyCredential(config.azure_ai_search_key)\n",
    "        )\n",
    "        results = search_client.search(query, top=2)\n",
    "        output = \"\\n\".join([doc[\"content\"] for doc in results])\n",
    "        logger.info(f\"Azure AI Search query: {query}\")\n",
    "        logger.info(f\"Azure AI Search results: {output}\")\n",
    "        return output or \"No results found.\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Azure AI Search error: {e}\")\n",
    "        return f\"Search failed: {e}\"\n",
    "\n",
    "# Define other tools\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate basic math expressions.\"\"\"\n",
    "    try:\n",
    "        allowed_chars = set('0123456789+-*/().** ')\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return \"Error: Only basic mathematical operations are allowed\"\n",
    "        logger.info(f\"Azure AI Search query: {expression}\")\n",
    "        logger.info(f\"Azure AI Search results: {allowed_chars}\")\n",
    "        return f\"Result: {eval(expression)}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Azure AI Search error: {e}\")\n",
    "        return f\"Error evaluating expression: {e}\"\n",
    "\n",
    "@tool \n",
    "def format_text(text: str, style: str = \"upper\") -> str:\n",
    "    \"\"\"Format text in upper, lower, or title case.\"\"\"\n",
    "    return {\n",
    "        \"upper\": text.upper(),\n",
    "        \"lower\": text.lower(),\n",
    "        \"title\": text.title()\n",
    "    }.get(style, f\"Unknown style: {style}\")\n",
    "\n",
    "# Combine tools\n",
    "tools = [azure_ai_search, calculate, format_text]\n",
    "\n",
    "# LangGraph agent setup\n",
    "system_message = \"You are a helpful assistant that can search enterprise content, calculate, and format text. Explain your reasoning step by step.\"\n",
    "memory = MemorySaver()\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm_agent,\n",
    "    tools=tools,\n",
    "    prompt=system_message,\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# Run a test query\n",
    "print(\"ü§ñ Testing Azure-centric LangGraph agent...\")\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-conversation\"}}\n",
    "query = \"Calculate 16 raised to the power of 0.5, then format the result as 'The answer is X' in title case\"\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [(\"user\", query)]},\n",
    "    config=config,\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    if step:\n",
    "        print(f\"üìù Step: {step}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ MODERN 2025 FEATURES DEMONSTRATED:\")\n",
    "print(\"   üîÑ LangGraph agent (replaces legacy AgentExecutor)\")\n",
    "print(\"   üíæ Built-in memory with conversation threading\")\n",
    "print(\"   üåä Real-time streaming execution\")\n",
    "print(\"   üîç Azure Application Insights tracing\")\n",
    "print(\"=\"* 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-integration",
   "metadata": {},
   "source": [
    "## üéØ 2025 Evaluation & Monitoring Integration\n",
    "\n",
    "**What this does:** Integrates production-grade evaluation and monitoring using the leading 2025 frameworks to ensure AI application quality and performance.\n",
    "\n",
    "The code below demonstrates:\n",
    "- **Azure Machine Learning + Prompt Flow integration** - for LLM evaluation and metrics\n",
    "- **Azure Application Insights monitoring** - production tracing and evaluation for LangChain applications\n",
    "- **Automated evaluation metrics** including Answer Relevancy, Faithfulness, and custom scoring\n",
    "- **Cost tracking and token usage** monitoring for production optimization\n",
    "- **Unit test-style evaluation** using pytest-compatible DeepEval patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "evaluation-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azureml-mlflow 1.60.0 requires azure-storage-blob<=12.19.0,>=12.5.0, but you have azure-storage-blob 12.26.0 which is incompatible.\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Azure ML client initialized\n",
      "üìä Workspace: ml-workspace-pcm\n"
     ]
    }
   ],
   "source": [
    "# 2025 PATTERN: Modern evaluation framework integration\n",
    "%pip install --quiet azure.ai.ml \n",
    "\n",
    "import os\n",
    "import logging\n",
    "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Setup Azure Application Insights logging\n",
    "app_insights_conn = os.getenv(\"APPINSIGHTS_CONNECTION_STRING\")\n",
    "if app_insights_conn:\n",
    "    logger = logging.getLogger(\"azure_monitoring\")\n",
    "    logger.addHandler(AzureLogHandler(connection_string=app_insights_conn))\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.info(\"‚úÖ Azure Application Insights logging enabled\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Application Insights not configured (add APPINSIGHTS_CONNECTION_STRING to enable)\")\n",
    "\n",
    "# Setup Azure ML client for Prompt Flow and experiment tracking\n",
    "try:\n",
    "    ml_client = MLClient(\n",
    "        credential=DefaultAzureCredential(),\n",
    "        subscription_id=os.getenv(\"AZURE_ML_SUBSCRIPTION_ID\"),\n",
    "        resource_group=os.getenv(\"AZURE_ML_RESOURCE_GROUP\"),\n",
    "        workspace_name=os.getenv(\"AZURE_ML_WORKSPACE\")\n",
    "    )\n",
    "    print(\"‚úÖ Azure ML client initialized\")\n",
    "    print(f\"üìä Workspace: {ml_client.workspace_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize Azure ML client: {e}\")\n",
    "    print(\"Please ensure AZURE_ML_SUBSCRIPTION_ID, AZURE_ML_RESOURCE_GROUP, and AZURE_ML_WORKSPACE are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deepeval-integration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azure-storage-file-datalake 12.21.0 requires azure-storage-blob>=12.26.0, but you have azure-storage-blob 12.19.0 which is incompatible.\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Evaluation logged to Azure ML via MLflow\n",
      "‚úÖ Evaluation logged to Azure ML via MLflow\n",
      "üèÉ View run happy_flower_2hhwpwd2 at: https://swedencentral.api.azureml.ms/mlflow/v2.0/subscriptions/b0e6535c-d468-4bf0-81c2-4812406b7754/resourceGroups/rg-sie-energy-pcm-workshop-001/providers/Microsoft.MachineLearningServices/workspaces/ml-workspace-pcm/#/experiments/94eae68a-0fbf-4b93-9d5c-2f23ecd93429/runs/4700746e-3a3b-4f37-a72e-3bb5fa190878\n",
      "üß™ View experiment at: https://swedencentral.api.azureml.ms/mlflow/v2.0/subscriptions/b0e6535c-d468-4bf0-81c2-4812406b7754/resourceGroups/rg-sie-energy-pcm-workshop-001/providers/Microsoft.MachineLearningServices/workspaces/ml-workspace-pcm/#/experiments/94eae68a-0fbf-4b93-9d5c-2f23ecd93429\n",
      "üèÉ View run happy_flower_2hhwpwd2 at: https://swedencentral.api.azureml.ms/mlflow/v2.0/subscriptions/b0e6535c-d468-4bf0-81c2-4812406b7754/resourceGroups/rg-sie-energy-pcm-workshop-001/providers/Microsoft.MachineLearningServices/workspaces/ml-workspace-pcm/#/experiments/94eae68a-0fbf-4b93-9d5c-2f23ecd93429/runs/4700746e-3a3b-4f37-a72e-3bb5fa190878\n",
      "üß™ View experiment at: https://swedencentral.api.azureml.ms/mlflow/v2.0/subscriptions/b0e6535c-d468-4bf0-81c2-4812406b7754/resourceGroups/rg-sie-energy-pcm-workshop-001/providers/Microsoft.MachineLearningServices/workspaces/ml-workspace-pcm/#/experiments/94eae68a-0fbf-4b93-9d5c-2f23ecd93429\n"
     ]
    }
   ],
   "source": [
    "# 2025 PATTERN: Azure ML integration for comprehensive testing\n",
    "%pip install --quiet mlflow azureml-mlflow azureml azureml-core\n",
    "import os\n",
    "import mlflow\n",
    "from azure.identity import AzureCliCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Authenticate and connect to Azure ML workspace\n",
    "ml_client = MLClient(\n",
    "    credential=AzureCliCredential(),\n",
    "    subscription_id=os.getenv(\"AZURE_ML_SUBSCRIPTION_ID\"),\n",
    "    resource_group=os.getenv(\"AZURE_ML_RESOURCE_GROUP\"),\n",
    "    workspace_name=os.getenv(\"AZURE_ML_WORKSPACE\")\n",
    ")\n",
    "\n",
    "# Set MLflow tracking URI manually\n",
    "tracking_uri = os.getenv(\"AZURE_MLFLOW_TRACKING_URI\") \n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment(\"LangGraph-Evaluation\")\n",
    "\n",
    "# Simulated evaluation logic\n",
    "input_text = \"What are the main benefits of LangGraph?\"\n",
    "actual_output = \"LangGraph provides state management, complex agent workflows, and streaming capabilities for multi-agent systems.\"\n",
    "expected_output = \"LangGraph offers state management and multi-agent orchestration capabilities.\"\n",
    "context = [\"LangGraph is LangChain's framework for building stateful, multi-agent applications\"]\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"input\", input_text)\n",
    "    mlflow.log_param(\"expected_output\", expected_output)\n",
    "    mlflow.log_param(\"actual_output\", actual_output)\n",
    "    mlflow.log_param(\"context\", str(context))\n",
    "\n",
    "    # Simulated scoring logic\n",
    "    relevancy_score = 0.85\n",
    "    faithfulness_score = 0.9\n",
    "\n",
    "    mlflow.log_metric(\"relevancy\", relevancy_score)\n",
    "    mlflow.log_metric(\"faithfulness\", faithfulness_score)\n",
    "\n",
    "    print(\"‚úÖ Evaluation logged to Azure ML via MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc330cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "üí∞ Cost Analysis:\n",
      "  Total Tokens: 334\n",
      "  Prompt Tokens: 135\n",
      "  Completion Tokens: 199\n",
      "  Total Cost: $0.0019\n",
      "\n",
      "ü§ñ Agent Response: In May, New York (specifically New York City) typically experiences mild to warm spring weather. Here‚Äôs a general overview of the climate during this month:\n",
      "\n",
      "- **Average High Temperature:** 68¬∞F (20¬∞C)\n",
      "- **Average Low Temperature:** 53¬∞F (12¬∞C)\n",
      "- **Rainfall:** May is moderately wet, with an average of about 4 inches (100 mm) of rain spread over 11-12 days.\n",
      "- **Humidity:** Humidity levels are comfortable, not as high as in the summer.\n",
      "- **Daylight:** Days are longer, with sunrise around 5:30-6:00 AM and sunset around 8:00 PM by the end of the month.\n",
      "- **Weather:** The weather is generally pleasant, with a mix of sunny and cloudy days. Occasional rain showers are common, but prolonged rain is rare.\n",
      "\n",
      "May is considered one of the best months to visit New York due to the comfortable temperatures and blooming parks.\n",
      "üìä Performance Metrics: {'total_tokens': 334, 'cost': 0.0018620000000000002, 'prompt_tokens': 135, 'completion_tokens': 199}\n",
      "\n",
      "üí∞ Cost Analysis:\n",
      "  Total Tokens: 334\n",
      "  Prompt Tokens: 135\n",
      "  Completion Tokens: 199\n",
      "  Total Cost: $0.0019\n",
      "\n",
      "ü§ñ Agent Response: In May, New York (specifically New York City) typically experiences mild to warm spring weather. Here‚Äôs a general overview of the climate during this month:\n",
      "\n",
      "- **Average High Temperature:** 68¬∞F (20¬∞C)\n",
      "- **Average Low Temperature:** 53¬∞F (12¬∞C)\n",
      "- **Rainfall:** May is moderately wet, with an average of about 4 inches (100 mm) of rain spread over 11-12 days.\n",
      "- **Humidity:** Humidity levels are comfortable, not as high as in the summer.\n",
      "- **Daylight:** Days are longer, with sunrise around 5:30-6:00 AM and sunset around 8:00 PM by the end of the month.\n",
      "- **Weather:** The weather is generally pleasant, with a mix of sunny and cloudy days. Occasional rain showers are common, but prolonged rain is rare.\n",
      "\n",
      "May is considered one of the best months to visit New York due to the comfortable temperatures and blooming parks.\n",
      "üìä Performance Metrics: {'total_tokens': 334, 'cost': 0.0018620000000000002, 'prompt_tokens': 135, 'completion_tokens': 199}\n"
     ]
    }
   ],
   "source": [
    "# 2025 PATTERN: Cost tracking and token usage monitoring\n",
    "%pip install --quiet langchain_community\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "# Setup Azure Application Insights logging\n",
    "logger = logging.getLogger(\"azure_agent_cost_logger\")\n",
    "logger.addHandler(AzureLogHandler(connection_string=os.getenv(\"APPINSIGHTS_CONNECTION_STRING\")))\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def run_agent_with_cost_tracking(agent, query):\n",
    "    \"\"\"Run agent with comprehensive cost and performance tracking\"\"\"\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        # Configure conversation with thread ID for tracking\n",
    "        config = {\"configurable\": {\"thread_id\": \"cost-tracking-demo\"}}\n",
    "        \n",
    "        # Run the agent with streaming\n",
    "        response = None\n",
    "        for chunk in agent.stream({\"messages\": [(\"human\", query)]}, config):\n",
    "            if \"agent\" in chunk:\n",
    "                response = chunk[\"agent\"][\"messages\"][-1].content\n",
    "\n",
    "\n",
    "     # Log to Application Insights\n",
    "    logger.info(\"Agent run completed\", extra={\n",
    "        \"custom_dimensions\": {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"total_tokens\": cb.total_tokens,\n",
    "            \"prompt_tokens\": cb.prompt_tokens,\n",
    "            \"completion_tokens\": cb.completion_tokens,\n",
    "            \"estimated_cost_usd\": cb.total_cost\n",
    "        }\n",
    "    })\n",
    "   \n",
    "    # Display cost metrics\n",
    "    print(\"\\nüí∞ Cost Analysis:\")\n",
    "    print(f\"  Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"  Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"  Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"  Total Cost: ${cb.total_cost:.4f}\")\n",
    "    \n",
    "    return response, {\n",
    "        'total_tokens': cb.total_tokens,\n",
    "        'cost': cb.total_cost,\n",
    "        'prompt_tokens': cb.prompt_tokens,\n",
    "        'completion_tokens': cb.completion_tokens\n",
    "    }\n",
    "\n",
    "# Example usage with cost tracking\n",
    "if 'agent' in locals():\n",
    "    #test_query = \"What's the current weather in San Francisco?\"\n",
    "    test_query = \"What's the general climate information of New York during May?\"\n",
    "    response, metrics = run_agent_with_cost_tracking(agent, test_query)\n",
    "    \n",
    "    print(f\"\\nü§ñ Agent Response: {response}\")\n",
    "    print(f\"üìä Performance Metrics: {metrics}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Agent not initialized - run the previous cells first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0eb7c8",
   "metadata": {},
   "source": [
    "## üéØ Azure Application Insights for Kusto Query\n",
    "\n",
    "**Kusto Queries:** - access the Azure Application Insights Logs and run kusto queries to retrieve metrics \n",
    "- open Azure Application Insights\n",
    "- on the left, click on Logs.\n",
    "- click on New Query\n",
    "- select KQL Mode\n",
    "\n",
    "**Run the kusto query**:\n",
    "-     traces\n",
    "-        | where customDimensions contains \"estimated_cost_usd\"\n",
    "-        | project timestamp, customDimensions.query, customDimensions.total_tokens, customDimensions.estimated_cost_usd\n",
    "-        | order by timestamp desc\n",
    "\n",
    "**Result**:\n",
    "- timestamp [UTC] = 2025-09-16T23:56:09.550352Z\n",
    "- customDimensions_query = What's the general climate information of San Francisco during May?\n",
    "- customDimensions_total_tokens = 502\n",
    "- customDimensions_estimated_cost_usd = 0.0022040000000000002\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Reference: Kusto Query Language overview**:\n",
    "- https://learn.microsoft.com/en-us/kusto/query/?view=microsoft-fabric\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4e482",
   "metadata": {},
   "source": [
    "## üéØ Reference:\n",
    "\n",
    "\n",
    "\n",
    "**Microsoft Azure Monitor Opentelemetry Exporter Trace Python Samples**\n",
    "- https://learn.microsoft.com/en-us/samples/azure/azure-sdk-for-python/microsoft-azure-monitor-opentelemetry-exporter-trace-python-samples/\n",
    "\n",
    "**Monitor OpenAI Agents SDK with Application Insights:**:\n",
    "- https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/monitor-openai-agents-sdk-with-application-insights/4393949\n",
    "\n",
    "**Monitor Azure OpenAI**:\n",
    "- https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/monitor-openai\n",
    "\n",
    "**Integrating Azure Application Insights using opentelemetry-instrumentation in the Sample-app-aoai-chatGPT**:\n",
    "- https://github.com/microsoft/sample-app-aoai-chatGPT/issues/495\n",
    "\n",
    "**Visualize traces on Azure AI Foundry Tracing UI**:\n",
    "- https://learn.microsoft.com/en-us/semantic-kernel/concepts/enterprise-readiness/observability/telemetry-with-azure-ai-foundry-tracing\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y3bla5ukyb9",
   "metadata": {},
   "source": [
    "## Summary: LangChain Fundamentals (2025 Edition)\n",
    "\n",
    "This notebook covered the core concepts of LangChain using modern patterns:\n",
    "\n",
    "### Key Concepts Learned:\n",
    "1. **LCEL (LangChain Expression Language)**: Modern composition using pipe operators\n",
    "2. **Sequential Processing**: Chaining operations with result passing between steps\n",
    "3. **Streaming**: Real-time token streaming for better user experience  \n",
    "4. **LangGraph Agents**: Modern agent framework replacing legacy AgentExecutor\n",
    "5. **Evaluation Integration**: Azure Apllication Insights monitoring and Azure ML + Prompt Flow for testing frameworks\n",
    "6. **Cost Tracking**: Production-ready token usage and cost monitoring\n",
    "\n",
    "### 2025 LangChain Evolution:\n",
    "- **LangGraph**: State management and multi-agent orchestration\n",
    "- **Modern Agent Patterns**: Memory-enabled agents with conversation threading\n",
    "- **Evaluation Frameworks**: Comprehensive testing with Azure ML integration\n",
    "- **Production Monitoring**: Built-in cost tracking and performance metrics\n",
    "\n",
    "### 2025 Production Considerations:\n",
    "- Enable Azure Application Insights for tracing for production monitoring\n",
    "- Implement Azure ML for comprehensive agent testing\n",
    "- Use conversation threading for memory-enabled applications\n",
    "- Monitor costs with built-in callback handlers\n",
    "- Leverage LangGraph for complex multi-agent orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6a4de",
   "metadata": {},
   "source": [
    "## üéØ Appendix 1\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "An example to show an application using Opentelemetry tracing api and sdk. \n",
    "Custom dependencies are:\n",
    "- tracked via spans \n",
    "- telemetry is exported to application insights with the AzureMonitorTraceExporter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f49caa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Sure! Here you go:\n",
      "\n",
      "Why did the scarecrow win an award?  \n",
      "Because he was outstanding in his field!\n",
      "Sure! Here you go:\n",
      "\n",
      "Why did the scarecrow win an award?  \n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "%pip install --quiet openinference-instrumentation-langchain opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp-proto-http\n",
    "\n",
    "# mypy: disable-error-code=\"attr-defined\"\n",
    "import os\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()#'azure.env')\n",
    "\n",
    "exporter = AzureMonitorTraceExporter.from_connection_string(\n",
    "    os.getenv(\"APPINSIGHTS_CONNECTION_STRING\")\n",
    ")\n",
    "\n",
    "tracer_provider = TracerProvider()\n",
    "\n",
    "trace_api.set_tracer_provider(tracer_provider)\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "tracer = trace.get_tracer(__name__)\n",
    "span_processor = BatchSpanProcessor(exporter, schedule_delay_millis=60000)\n",
    "trace.get_tracer_provider().add_span_processor(span_processor)\n",
    "LangChainInstrumentor().instrument()\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "prompt = PromptTemplate(input_variables=[\"adjective\"], template=prompt_template)\n",
    "llm = AzureChatOpenAI(api_key = os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "                      azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'), \n",
    "                      api_version = '2024-06-01', \n",
    "                      model= os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME'))\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, metadata={\"category\": \"jokes\"})\n",
    "completion = chain.predict(adjective=\"funny\", metadata={\"variant\": \"funny\"})\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6358cc",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd53c6d",
   "metadata": {},
   "source": [
    "## üéØ Appendix 2\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "**Azure API MAnagement / MCP Servers / Policies**\n",
    "\n",
    "- Overview of AI gateway capabilities in Azure API Management\n",
    "https://learn.microsoft.com/en-us/azure/api-management/genai-gateway-capabilities\n",
    "\n",
    "- About MCP servers in Azure API Management\n",
    "https://learn.microsoft.com/en-us/azure/api-management/mcp-server-overview \n",
    "</br></br>\n",
    "  Limitations: \n",
    "</br>\n",
    "       Expose and govern an existing MCP server\n",
    "       https://learn.microsoft.com/en-us/azure/api-management/expose-existing-mcp-server#limitations\n",
    "</br>\n",
    "       Expose REST API in API Management as an MCP server\n",
    "</br>\n",
    "       https://learn.microsoft.com/en-us/azure/api-management/export-rest-mcp-server#limitations\n",
    "\n",
    "- Secure access to MCP servers in API Management\n",
    "https://learn.microsoft.com/en-us/azure/api-management/secure-mcp-servers#steps-to-configure-oauth-2-based-outbound-access\n",
    "\n",
    "- MCP Center\n",
    "https://mcp.azure.com/\n",
    "\n",
    "- MCP Inspector\n",
    "https://modelcontextprotocol.io/docs/tools/inspector\n",
    "\n",
    "- APIM ‚ù§Ô∏è MCP\n",
    "https://github.com/Azure-Samples/AI-Gateway/tree/main/labs/mcp-client-authorization\n",
    "\n",
    "- Policies in Azure API Management\n",
    "https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-policies\n",
    "\n",
    "- Protect your API\n",
    "https://learn.microsoft.com/en-us/azure/api-management/transform-api\n",
    "\n",
    "- Policies to validate header context\n",
    "https://learn.microsoft.com/en-us/azure/api-management/secure-mcp-servers\n",
    "\n",
    "- Use named values in Azure API Management policies\n",
    "https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-properties?tabs=azure-portal\n",
    "\n",
    "-  Mock API responses\n",
    "https://learn.microsoft.com/en-us/azure/api-management/mock-api-responses?tabs=azure-portal\n",
    "\n",
    "- Debug your APIs using request tracing\n",
    "https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-api-inspector?WT.mc_id=Portal-Microsoft_Azure_ApiManagement#enable-tracing-for-an-api\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
